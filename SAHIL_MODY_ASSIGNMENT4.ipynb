{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>HW 4: Text preprocessing</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NAme: Sahil Mahendra Mody\n",
    "#Assignment:4\n",
    "#CWID:20007262\n",
    "#SUbject: Web mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, json, string\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Regular Expression\n",
    "\n",
    "Suppose you have scraped the text shown below from an online source. You'd like to extract data using regular expression.\n",
    "\n",
    "Define a **extract** function which:\n",
    "- Takes a piece of text (in the format of shown below) as an input\n",
    "- Extracts data into a list of tuples using regular expression, e.g.  `[('BTC-USD','56,212.15','-58.16','-0.10%'), ('ETH-USD',  ...), ...]`\n",
    "- Returns the list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Symbol   Last Price  Change   % Change   Note\\n                  BTC-USD  56,212.15   -58.16   -0.10%   Bitcoin \\n                  ETH-USD  1,787.79    -53.63   -2.91%   Ether\\n                  BNB-USD  1,101,290.51      +5.81    +2.04%   Binance\\n                  USDT-USD 1.0003      -0.0004  -0.04%   Tether\\n                  ADA-USD  1.1187      -0.0528  -4.51%   Cardano\\n      '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='''Symbol   Last Price  Change   % Change   Note\n",
    "                  BTC-USD  56,212.15   -58.16   -0.10%   Bitcoin \n",
    "                  ETH-USD  1,787.79    -53.63   -2.91%   Ether\n",
    "                  BNB-USD  1,101,290.51      +5.81    +2.04%   Binance\n",
    "                  USDT-USD 1.0003      -0.0004  -0.04%   Tether\n",
    "                  ADA-USD  1.1187      -0.0528  -4.51%   Cardano\n",
    "      '''\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "import re\n",
    "def extract(text):\n",
    "    #b=re.findall(r'(\\w+-\\w+)\\s+(\\d+.+\\d%)',text)\n",
    "    b=re.findall(r'(\\w+-\\w+)\\s+([\\d+,.]*)\\s+([-\\d+.]*)\\s+([-\\d+.%]*)',text)\n",
    "    return b\n",
    "# add your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BTC-USD', '56,212.15', '-58.16', '-0.10%'),\n",
       " ('ETH-USD', '1,787.79', '-53.63', '-2.91%'),\n",
       " ('BNB-USD', '1,101,290.51', '+5.81', '+2.04%'),\n",
       " ('USDT-USD', '1.0003', '-0.0004', '-0.04%'),\n",
       " ('ADA-USD', '1.1187', '-0.0528', '-4.51%')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "#lst_tuple = list(zip(lst1,lst2))\n",
    "extract(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Collocation\n",
    "\n",
    "Define a function `top_collocation(doc, K)` to find top-K collocations in specific patterns in a document as follows:\n",
    "  - Takes a document (i.e. `doc`) and `K` as inputs\n",
    "  - Find collocations as follows:\n",
    "    - Tokenize the document and find POS tag of each token (hint: you can use NLTK word tokenizer or Spacy tokenizer).\n",
    "    - Create bigrams from the tokens with POS tags and remove a bigram if one of the bigram is punctuation.\n",
    "\n",
    "    - Keep only bigrams matching the following patterns:\n",
    "       - `Adj + Noun`: e.g. linear function\n",
    "       - `Noun + Noun`: e.g. regression coefficient\n",
    "    - Get frequency of each bigram (hint: you can use nltk.FreqDist)\n",
    "    - Returns top K collocations by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "\n",
    "\n",
    "def top_collocation(doc, K):\n",
    "    #tokenizing\n",
    "    tokens=nltk.word_tokenize(doc)\n",
    "    punctuations=set(list(string.punctuation))\n",
    "    #Striping punctuations\n",
    "    strip=[token.strip() for token in tokens if token.lower() not in punctuations]\n",
    "\n",
    "# tag each tokenized word\n",
    "    tagged_tokens= nltk.pos_tag(strip)\n",
    "    bigrams=list(nltk.bigrams(tagged_tokens))\n",
    "    grams=[(x,y)for(x,y) in bigrams if x[0] not in punctuations and y[0] not in punctuations]\n",
    "    #phrases1 for adjective + noun\n",
    "    phrases1=[ (x[0],y[0]) for (x,y) in grams \\\n",
    "         if x[1].startswith('JJ') \\\n",
    "         and y[1].startswith('NN')]\n",
    "    #phrases2 for noun + noun\n",
    "    phrases2=[ (x[0],y[0]) for (x,y) in grams \\\n",
    "         if x[1].startswith('NN') \\\n",
    "         and y[1].startswith('NN')]\n",
    "    phrases=phrases1+phrases2\n",
    "    token_count=nltk.FreqDist(phrases)\n",
    "\n",
    "\n",
    "    # add your code\n",
    "    return token_count.most_common(K)\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('public', 'health'), 14),\n",
       " (('community', 'spread'), 9),\n",
       " (('United', 'States'), 8),\n",
       " (('higher', 'risk'), 4),\n",
       " (('COVID-19', 'illness'), 4),\n",
       " (('elevated', 'risk'), 4)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json.load(open(\"C:/Users/SAHIL MODY/OneDrive/Desktop/Web Mining/qa.json\",\"r\"))\n",
    "article = data[\"context\"]\n",
    "\n",
    "top_collocation(article, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Question and Answering (QA) System\n",
    "\n",
    "Develop a QA system which allow you to search for answers in an article. For example, the file `qa.json` contains a research article. This article can answer a number of questions about COVID-19. You will design a solution to automatically search answers to these questions in this article.\n",
    "\n",
    "`qa.json` is taken from https://github.com/deepset-ai/COVID-QA. This file contains a few questions, and answers to these questions have been located in the article. Let's define a QA system and check if your system can locate the right answers.\n",
    "\n",
    "The following script helps you understand `qa.json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDC Summary 21 MAR 2020,\n",
      "https://www.cdc.gov/coronavirus/2019-ncov/cases-updates/summary.html\n",
      "\n",
      "This is a rapidly evolving situation and CDC will provide updated information and guidance as it becomes \n"
     ]
    }
   ],
   "source": [
    "# Retrieve the article\n",
    "\n",
    "data = json.load(open(\"C:/Users/SAHIL MODY/OneDrive/Desktop/Web Mining/qa.json\",\"r\"))\n",
    "article = data[\"context\"]\n",
    "\n",
    "# A long article. Just print the first 200 characters\n",
    "print(article[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What age group has the highest rate of severe outcomes?', 'id': 236, 'answers': [{'text': 'people 85 years and older', 'answer_start': 6117}], 'is_impossible': False}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['What age group has the highest rate of severe outcomes?',\n",
       " 'How is COVID-19 spread?',\n",
       " 'How many states in the U.S. have reported cases of COVID-19?',\n",
       " 'When did the White House launch the \"15 Days to Slow the Spread\" program?',\n",
       " 'What should mildly-ill patients do?',\n",
       " 'What type of virus is SARS-CoV-2?',\n",
       " 'What viruses are similar to the COVID-19 coronavirus?',\n",
       " 'What are the phases of a pandemic?',\n",
       " 'At which phase does the peak of the pandemic occur?',\n",
       " 'People with which medical conditions have a higher rate of severe illness?',\n",
       " 'What kind of test can diagnose COVID-19?',\n",
       " 'In what species did the COVID-19 virus likely originate?',\n",
       " 'What risk factors should be considered in addition to clinical symptoms?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve all the questions and answers\n",
    "qas = data[\"qas\"]\n",
    "\n",
    "# show the first question-answer pair. Note the answer starts at the 6117th character\n",
    "print(qas[0])\n",
    "\n",
    "# get all questions\n",
    "qs = [item[\"question\"] for item in qas]\n",
    "qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, following the instructions below step by step to develop the QA system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.1. Tokenizer\n",
    "\n",
    "Define a function `tokenize(doc)`  as follows:\n",
    "   - Take a piece of text (i.e. variable `doc`) as an input\n",
    "   - Split the input text into unigrams\n",
    "   - Clean up tokens as follows:\n",
    "       - Lemmatize all unigrams\n",
    "       - Remove all stop words\n",
    "       - Remove all punctuations\n",
    "       - Convert all unigrams to the lower case \n",
    "       - remove empty unigrams\n",
    "   - Return the list of unigrams after all the processing. (Hint: you can use spacy or stanza package for this task. To test if a token is stop word or punctuation, check https://spacy.io/api/token#attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\SAHIL\n",
      "[nltk_data]     MODY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "\n",
    "import string\n",
    "def tokenize(doc):\n",
    "    #tokens=doc.split(' ')\n",
    "    punctuations=\"?:!.,;—\"\n",
    "    #tokeinizing\n",
    "    sentence_words = nltk.word_tokenize(doc)\n",
    "    hello=[]\n",
    "    #sentence_words =[token.strip(string.punctuation+\"—\") for token in sentence_words]\n",
    "    #removing punctuations\n",
    "    for word in sentence_words:\n",
    "        if word in punctuations:\n",
    "            sentence_words.remove(word)\n",
    "    tokens_without_sw = [word for word in sentence_words if not word in stopwords.words()]\n",
    "    #converting to lower case\n",
    "    for i in range(len(tokens_without_sw)):\n",
    "        tokens_without_sw[i]=tokens_without_sw[i].lower()\n",
    "    #Lemmatization\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(len(tokens_without_sw)) :\n",
    "        a=lemmatizer.lemmatize(tokens_without_sw[i],pos='a')\n",
    "        hello.append(a)\n",
    "       \n",
    "                       \n",
    "    return hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['old', 'people', 'people', 'ages', 'severe', 'chronic', 'medical', 'conditions', 'like', 'heart', 'disease', 'lung', 'disease', 'diabetes', 'example', 'seem', 'high', 'risk', 'developing', 'serious', 'covid-19', 'illness']\n"
     ]
    }
   ],
   "source": [
    "doc = 'Older people and people of all ages with severe chronic medical conditions — \\\n",
    "like heart disease, lung disease and diabetes, \\\n",
    "for example — seem to be at higher risk of developing serious COVID-19 illness.'\n",
    "\n",
    "print(tokenize(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.2. Compute TF-IDF Matrix\n",
    "\n",
    "Define a function `compute_tfidf(docs)` as follows: \n",
    "\n",
    "- Take `docs`, a list of documents (e.g. a list of questions) as an input\n",
    "- Tokenize each document in `docs` using the `tokenize` function defined in Q3.1. \n",
    "- Calculate tf_idf weights as shown in lecture notes (Hint: you can reuse the last code segment in NLP Lecture Notes (II))\n",
    "- Return a smoothed normalized `tf_idf` array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def get_doc_tokens(doc):\n",
    "    tokens=nltk.word_tokenize(doc.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens_strip=[token.strip() \\\n",
    "            for token in tokens\n",
    "            if token.strip() not in stop_words and token.strip() not in string.punctuation]\n",
    "    \n",
    "    # you can add bigrams, collocations, stemming, \n",
    "    # or lemmatization here\n",
    "    \n",
    "    token_count={token:tokens_strip.count(token) for token in set(tokens_strip)}\n",
    "    return token_count\n",
    "\n",
    "def compute_tfidf(docs):\n",
    "    # step 2. process all documents to get list of token list\n",
    "    docs_tokens={idx:get_doc_tokens(doc) \\\n",
    "             for idx,doc in enumerate(qs[0:3])}\n",
    "\n",
    "    # step 3. get document-term matrix\n",
    "    dtm=pd.DataFrame.from_dict(docs_tokens, orient=\"index\" )\n",
    "    dtm=dtm.fillna(0)\n",
    "    dtm = dtm.sort_index(axis = 0)\n",
    "      \n",
    "    # step 4. get normalized term frequency (tf) matrix        \n",
    "    tf=dtm.values\n",
    "    doc_len=tf.sum(axis=1, keepdims=True)\n",
    "    tf=np.divide(tf, doc_len)\n",
    "    \n",
    "    # step 5. get idf\n",
    "    df=np.where(tf>0,1,0)\n",
    "    #idf=np.log(np.divide(len(docs), \\\n",
    "    #    np.sum(df, axis=0)))+1\n",
    "\n",
    "    smoothed_idf=np.log(np.divide(len(docs)+1, np.sum(df, axis=0)+1))+1    \n",
    "    smoothed_tf_idf=normalize(tf*smoothed_idf)\n",
    "    \n",
    "    return smoothed_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41, 0.41, 0.41, 0.41, 0.41, 0.41, 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.8 , 0.61, 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.32, 0.42, 0.42, 0.42,\n",
       "        0.42, 0.42]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "compute_tfidf(qs[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.2. Put Everything Together (bonus question)\n",
    "\n",
    "Define a function `find_solutions(qs, article)` as follows: \n",
    "\n",
    "- Take two inputs:\n",
    "    - `qs`: a list of questions (i.e. strings)\n",
    "    - `article`: a document which may contain answers to the questions\n",
    "- Segment the article into sentences (i.e. `sents`). You will locate the sentence which can answer a question.\n",
    "- Concatenate the questions (`qs`) and sentences (`sents`) into a single list (i.e. `qs + sents`)\n",
    "- Call the function `compute_tfidf` defined in Q3.2 with `qs + sents` to get a `TF-IDF` matrix. (Note, now `qs` and `sents` are converted to TF-IDF vectors in the same dimension. As a result, you can measure their similarities.) \n",
    "- Split the `TF-IDF` matrix into two sub matrices, one corresponding to `qs` and the other for `sents`. \n",
    "- Next, calculate the pairwise cosine similarity between the `qs` and `sents`. With $m$ questions and $n$ sentences, you should get a $m \\times n$ matrix. (hint: you can `sklearn.metrics.pairwise_distances` to calculate pairwise distances between two matrices)\n",
    "- Finally, the answer to each question is the sentence which has the `maximum similarity` to it. \n",
    "- Print out each question and its matched answer. Check if your QA system is able to find the right answer.(Depending on the packages you use, your answer might be different from mine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "import sklearn\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "def find_solutions(qs, article):\n",
    "    \n",
    "    \n",
    "    return mylist\n",
    "    # add your code\n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mylist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-c9c553167937>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Test the system\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfind_solutions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marticle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-7c985a042e73>\u001b[0m in \u001b[0;36mfind_solutions\u001b[1;34m(qs, article)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmylist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;31m# add your code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mylist' is not defined"
     ]
    }
   ],
   "source": [
    "# Test the system\n",
    "\n",
    "find_solutions(qs, article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
